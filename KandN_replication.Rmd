---
title: "Replicating King & Nielsen (2019)"
author: "Noah Greifer"
date: "2/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
source("misc.R")
```

This is a document to demonstrate that the results of King & Nielsen (2019) hold whether the implied DAG for the data-generating process (DGP) represents a mediated treatment effect or a confounded treatment effect. First, I demonstrate how to generate the K&N data using a model in which the covariates `X1` and `X2` cause selection into treatment `A`. This data is indistinguishable from that generated using K&N's DGP but is consistent with a DAG representing confounding, while K&N's DAG represents mediation.

Generating K&N's data under a confounding DAG is actually quite difficult. I first create an exogenous auxiliary variable `G1`, from which I can generate `X1` and `X2`, which then cause selection into treatment. Note that there is a positivity violation in that several units get propensity scores of 0 or 1.

```{r}
#All this to replicate covariate + treatment distribution of K&N but
#generating predictors first and then treatment to ensure DGP is
#faithful to implied DAG (i.e., w/ confounders rather than mediators)
n = 10000
G1 = sample(LETTERS[1:5], n, T, prob = c(5/50, 4/50, 32/50, 4/50, 5/50))
X1 = runif(n, 
           ifelse_(G1=="A", 1, G1=="B", 0, G1=="C", 1, G1=="D", 5, 0), 
           ifelse_(G1=="A", 6, G1=="B", 1, G1=="C", 5, G1=="D", 6, 5))
X2 = runif(n, 
           ifelse_(G1=="A", 5, G1=="B", 1, G1=="C", 1, G1=="D", 1, 0), 
           ifelse_(G1=="A", 6, G1=="B", 5, G1=="C", 5, G1=="D", 5, 1))

#P is propensity score.
P = 1*(X1 > 1)*(X2 > 1) - .5*(X1 > 1)*(X1 < 5)*(X2 > 1)*(X2 < 5)

#Have to use sample() rather than rbinom to ensure exactly 100 T and 
#100 C are assigned. Could use rbinom if this isn't imperative,
#but doing so makes it harder to standardize the number of dropped
#pairs accross replications. If using sample, P(A=1) depends on who
#was assigned to treatment before, so P =/= PS for "last" units to be
#assigned.

#A <- rbinom(n, 1, P)
A <- rep(0, n)
A[P == 1] <- 1
A[P == .5] <- sample(c(rep(1, n/2 - sum(P==1)), rep(0, n/2 - sum(P==0))), sum(P == .5), FALSE)

dng <- data.frame(A, X1, X2, data_method = "ng")

A <- sample(rep(0:1, each = n/2), n, FALSE)
X1 <- ifelse(A==1, runif(n, 1,6), runif(n, 0,5))
X2 <- ifelse(A==1, runif(n, 1,6), runif(n, 0,5))

dkn <- data.frame(A, X1, X2, data_method = "kn")

d <- rbind(dkn, dng)

library(ggplot2)
ggplot() + geom_point(data = d[c(1:300, n + 1:300),], 
                      aes(x = X1, y = X2, color = factor(A))) +
    facet_wrap(vars(data_method)) + 
    geom_rect(data = data.frame(min = c(0, 1),
                                max = c(5, 6),
                                A = factor(0:1)),
              mapping = aes(xmin = min, ymin = min, 
                            xmax = max, ymax = max, 
                            fill = A), alpha = .1) + 
    jtools::theme_apa()
```

The graphs above indicate that the dataset generated using the confounding DAG (NG) DGP is indistinguishable to the one generated using K&N's DGP. Below, we also see that the correlations among the variables are nearly identical (this is especially true with large samples).
```{r}
cor(dkn[-4])-cor(dng[-4])
```

Below I attempt to replicate the results of K&N using both DGPs. The code for `est_rep`, which estimates the treatment effects, is in `misc.R`. You might want to examine it to ensure I have adequately represented the estimation methods. For PS matching, I use a logistic regression propensity score. I use nearest neighbor matching without replacement and prune pairs of units one by one, starting with the pairs that have the greatest distance between the members. For each matching schema, I fit K&N's 512 outcome models to the matched sets and compute the effect estimates. Then I compute the variance of the effect estimates, which represent model dependence; the largest effect estimate, which represents what a naughty researcher would pick; and the simple difference in means in the matched set, which, to me, represents the most common practice.

```{r}
nrep <- 200
res_kn <- lapply(1:nrep, est_rep, method = "kn", n = 200)
res_ng <- lapply(1:nrep, est_rep, method = "ng", n = 200)

out <- get_out(kn = res_kn, ng = res_ng)
```

Below are the results of the analysis, mimicking the top of figure 2 in K&N. First I plot the variance of the 512 effect estimates against the number of pairs pruned (note K&N use the number of units pruned, but we prune one pair of units at a time). Each point on the line corresponds to the average of the variance across the simulation replications. I used loess smoothing to make the patterns clearer. Here we can see that the PS paradox is in effect; at a certain point, pruning more units increases the model dependence when using PS matching, but not with Mahalanobis distance matching.
```{r}
ggplot(out, aes(x = ind, y = variance, color = method)) + geom_smooth(se = FALSE) +
    facet_grid(col = vars(data_method)) + theme(legend.position = "bottom")
```

Below we have the maximum estimated effect for each method. It's not clear that the PS paradox is in effect. I am unable to completely replicate K&N's findings.
```{r}
ggplot(out, aes(x = ind, y = max_est, color = method)) + geom_smooth(se = FALSE) +
    facet_grid(col = vars(data_method)) + theme(legend.position = "bottom") +
    geom_hline(yintercept = 2)
```

Finally, a plot not shown in K&N, which is whether the PS paradox affects the simple difference in means. It appears not to, indicating that perhaps no outcome model is better than fishing around for a model. That said, because the PS model used does not correspond to the DGP, balance is never achieved by PS matching and the effect estimate remains biased. 
```{r}
ggplot(out, aes(x = ind, y = est, color = method)) + geom_smooth(se = FALSE) +
    facet_grid(col = vars(data_method)) + theme(legend.position = "bottom") +
    geom_hline(yintercept = 2)
```

We can look at the imbalance in `X1` and `X2` to see if this tells a story.

```{r}
ggplot(out, aes(x = ind, y = X1_imbal, color = method)) + geom_smooth(se = FALSE) +
    facet_grid(col = vars(data_method)) + theme(legend.position = "bottom") +
    geom_hline(yintercept = 0)
ggplot(out, aes(x = ind, y = X2_imbal, color = method)) + geom_smooth(se = FALSE) +
    facet_grid(col = vars(data_method)) + theme(legend.position = "bottom") +
    geom_hline(yintercept = 0)
```

Regardless of the measure, the results are the same whether K&N's DGP was used or the confounding DGP was used. This is because the performance of matching for estimating a regression coefficient doesn't relate to the causal status of the variables, all else (i.e., the joint distribution of variables) equal. Whether the covariates are confounders or mediators is irrelavant to the statistical performance of the estimator; it does, however, change the interpretation of the estimated effect. If the problem is instead framed as "use matching to estimate $\beta_{\tau}$ in the model $Y = \beta_{\tau}A + \beta_1 X_1 + \beta_2 X_2 + \epsilon$ where $Cov(A,X_1) \ne 0$ and $Cov(A,X_2) \ne 0$", it would be clearer that it doesn't matter what the causal status of any of the variables is. Matching eliminates the covariance between the $X$s and $A$ and allows for $Y~A$ to yield an unbiased estimate of $\beta_{\tau}$. No causal assumptions need to be invoked for a matching estimator that succeeds at balancing to be unbiased for the parameter it seeks to estimate.

I have problems with this paper for many other reasons than this one. I have already discussed how dangerous I think this paper is and has been. But I don't think its fault lies with the disconnect between the DAG implied by the text and the DGP used.